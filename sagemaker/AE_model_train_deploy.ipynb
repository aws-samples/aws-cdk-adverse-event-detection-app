{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-roman",
   "metadata": {},
   "source": [
    "# Train and Deploy a Transformer Model for Adverse Event Classification in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-thumbnail",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-scale",
   "metadata": {},
   "source": [
    "This notebook shows how to fine tune a transformer model in Amazon SageMaker for adverse event (AE) classification. We use the Hugging Face [Transformers](https://huggingface.co/transformers/) as example code and library to train and deploy the model in Amazon SageMaker.\n",
    "\n",
    "The AE dataset used in this demo is the Hugging Face's Adverse Drug Reaction Data: [ade_corpus_v2](https://huggingface.co/datasets/ade_corpus_v2). Users can replace the dataset with their own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-greenhouse",
   "metadata": {},
   "source": [
    "# Enviornment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "! pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-disabled",
   "metadata": {},
   "source": [
    "# 1. Raw dataset\n",
    "Here we use the Hugging Face's Adverse Drug Reaction to create a raw dateset for model training. Users can skip this step if they have their own raw dataset for AE classification. The raw data should have two columns: one is the text column, and the other is the class column to indicate whether a text mentions AE or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\")\n",
    "df_context, df_label = dataset['train'].__getitem__('text'), dataset['train'].__getitem__('label')\n",
    "df_raw = pd.DataFrame(\n",
    "    {'text': df_context,\n",
    "     'class': df_label\n",
    "    })\n",
    "\n",
    "# convert label id to class description for the raw dataset\n",
    "df_raw['class'] = df_raw['class'].apply(lambda x: 'Adverse_Event' if x == 1 else 'Not_AE')\n",
    "\n",
    "# Save the raw dataset to a local folder ./data/\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "df_raw.to_csv('./data/raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-palace",
   "metadata": {},
   "source": [
    "# 2. Process raw data and load it to S3 for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the raw data and convert the class names into integer IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    label2id = {'Adverse_Event': 1, 'Not_AE': 0}\n",
    "    df['label'] = df['class'].map(lambda x: label2id[x])\n",
    "    return df\n",
    "\n",
    "# load the raw data and do basic data processing\n",
    "df = load_data('./data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train, valid = train_test_split(df, test_size=0.20, shuffle = True, random_state = 1,  stratify=df[['class']])\n",
    "\n",
    "# save prepared train and valid datasets into local for S3 uploading\n",
    "data_dir = './data/model_input'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
    "valid.to_csv(os.path.join(data_dir, 'valid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-point",
   "metadata": {},
   "source": [
    "### Upload train/valid data to S3 for SageMaker model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'AE_bert/data'\n",
    "s3_prefix = 'HF_models/' + task_name\n",
    "\n",
    "# upload data to S3\n",
    "inputs_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=s3_prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-islam",
   "metadata": {},
   "source": [
    "# 3. SageMaker model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 4,\n",
    "                 'train_batch_size': 64,\n",
    "                 'max_seq_length': 128,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 'text_column':'text', # the column name for input text\n",
    "                 'label_column': 'label' # the column name for label IDs\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker PyTorch framework\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "bert_estimator = PyTorch(entry_point='hf_train_deploy.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type= train_instance_type, # use 'local' for code testing within the notebook instance\n",
    "                    hyperparameters = hyperparameters\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit({'training': inputs_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model artifact in S3 after training\n",
    "bert_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-market",
   "metadata": {},
   "source": [
    "# 4. SageMaker Endpoint Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = bert_estimator.model_data\n",
    "src_dir = 'src'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data,\n",
    "                             role=role,\n",
    "                             framework_version=\"1.4.0\",\n",
    "                             source_dir=src_dir,\n",
    "                             py_version=\"py3\",\n",
    "                             entry_point=\"hf_train_deploy.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, \n",
    "                                 instance_type=\"ml.m5.large\", \n",
    "                                 endpoint_name='HF-BERT-AE-model',\n",
    "                                 serializer=JSONSerializer(),\n",
    "                                 deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-realtor",
   "metadata": {},
   "source": [
    "# 5. Inference (optional): invoke SageMaker Endpoint\n",
    "This example shows how to invoke an endpoint for model prediction. You can use AWS Lambda to invoke the endpoint for real-time model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'HF-BERT-AE-model'\n",
    "runtime= boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'This entity is probably related to a combination of high doses of corticosteroids, vecuronium administration and metabolic abnormalities associated with respiratory failure.'\n",
    "\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(query))\n",
    "prob = eval(response['Body'].read())\n",
    "print(f\"probability for Not_AE: {round(prob[0],3)}, for AE: {round(prob[1],3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a classification threshold\n",
    "threshold = 0.6\n",
    "\n",
    "prd_prob = prob[1]\n",
    "pred_label = \"Adverse_Event\" if prd_prob >= threshold else \"Not_AE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label, prd_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-japanese",
   "metadata": {},
   "source": [
    "# 6. Cleanup (optional)\n",
    "\n",
    "If you don't need to keep the deployed endpoint live, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
